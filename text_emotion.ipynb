{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYZWMl-dXLdL",
        "outputId": "936414d1-8dc6-4bb7-c061-5315b4a2251e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<ipython-input-4-52cdd6ed37ec>:42: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['content'] = data['content'].str.replace(r'[^\\w\\s]',' ')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "naive bayes tfidf accuracy 0.48265895953757226\n",
            "svm using tfidf accuracy 0.5096339113680154\n",
            "log reg tfidf accuracy 0.4951830443159923\n",
            "random forest tfidf accuracy 0.4932562620423892\n",
            "naive bayes count vectors accuracy 0.7745664739884393\n",
            "lsvm using count vectors accuracy 0.7832369942196532\n",
            "log reg count vectors accuracy 0.7832369942196532\n",
            "random forest with count vectors accuracy 0.7495183044315993\n",
            "[0 0 0 0 1 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import nltk\n",
        "from nltk import download\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/text_emotion.csv')\n",
        "\n",
        "data = data.drop('author', axis=1)\n",
        "\n",
        "# Dropping rows with other emotion labels\n",
        "data = data.drop(data[data.sentiment == 'anger'].index)\n",
        "data = data.drop(data[data.sentiment == 'boredom'].index)\n",
        "data = data.drop(data[data.sentiment == 'enthusiasm'].index)\n",
        "data = data.drop(data[data.sentiment == 'empty'].index)\n",
        "data = data.drop(data[data.sentiment == 'fun'].index)\n",
        "data = data.drop(data[data.sentiment == 'relief'].index)\n",
        "data = data.drop(data[data.sentiment == 'surprise'].index)\n",
        "data = data.drop(data[data.sentiment == 'love'].index)\n",
        "data = data.drop(data[data.sentiment == 'hate'].index)\n",
        "data = data.drop(data[data.sentiment == 'neutral'].index)\n",
        "data = data.drop(data[data.sentiment == 'worry'].index)\n",
        "\n",
        "# Making all letters lowercase\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "# Removing Punctuation, Symbols\n",
        "data['content'] = data['content'].str.replace(r'[^\\w\\s]',' ')\n",
        "\n",
        "# Removing Stop Words using NLTK\n",
        "stop = stopwords.words('english')\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "#Lemmatisation\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "#Correcting Letter Repetitions\n",
        "\n",
        "def de_repeat(text):\n",
        "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
        "    return pattern.sub(r\"\\1\\1\", text)\n",
        "\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))\n",
        "\n",
        "# Code to find the top 10,000 rarest words appearing in the data\n",
        "freq = pd.Series(' '.join(data['content']).split()).value_counts()[-10000:]\n",
        "\n",
        "# Removing all those rarely appearing words from the data\n",
        "freq = list(freq.index)\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "\n",
        "#Encoding output labels 'sadness' as '1' & 'happiness' as '0'\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y = lbl_enc.fit_transform(data.sentiment.values)\n",
        "\n",
        "# Splitting into training and testing data in 90:10 ratio\n",
        "X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)\n",
        "\n",
        "# Extracting TF-IDF parameters\n",
        "tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf.fit_transform(X_val)\n",
        "\n",
        "# Extracting Count Vectors Parameters\n",
        "count_vect = CountVectorizer(analyzer='word')\n",
        "count_vect.fit(data['content'])\n",
        "X_train_count =  count_vect.transform(X_train)\n",
        "X_val_count =  count_vect.transform(X_val)\n",
        "\n",
        "# Model 1: Multinomial Naive Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "y_pred = nb.predict(X_val_tfidf)\n",
        "print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "# naive bayes tfidf accuracy 0.5289017341040463\n",
        "\n",
        "# Model 2: Linear SVM\n",
        "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
        "lsvm.fit(X_train_tfidf, y_train)\n",
        "y_pred = lsvm.predict(X_val_tfidf)\n",
        "print('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "# svm tfidf accuracy 0.5404624277456648\n",
        "\n",
        "# Model 3: logistic regression\n",
        "logreg = LogisticRegression(C=1)\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "y_pred = logreg.predict(X_val_tfidf)\n",
        "print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "# log reg tfidf accuracy 0.5443159922928709\n",
        "\n",
        "# Model 4: Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=500)\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "y_pred = rf.predict(X_val_tfidf)\n",
        "print('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "# random forest tfidf accuracy 0.5385356454720617\n",
        "\n",
        "## Building models using count vectors feature\n",
        "# Model 1: Multinomial Naive Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_count, y_train)\n",
        "y_pred = nb.predict(X_val_count)\n",
        "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "# naive bayes count vectors accuracy 0.7764932562620424\n",
        "\n",
        "# Model 2: Linear SVM\n",
        "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
        "lsvm.fit(X_train_count, y_train)\n",
        "y_pred = lsvm.predict(X_val_count)\n",
        "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "# lsvm using count vectors accuracy 0.7928709055876686\n",
        "\n",
        "# Model 3: Logistic Regression\n",
        "logreg = LogisticRegression(C=1)\n",
        "logreg.fit(X_train_count, y_train)\n",
        "y_pred = logreg.predict(X_val_count)\n",
        "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "# log reg count vectors accuracy 0.7851637764932563\n",
        "\n",
        "# Model 4: Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=500)\n",
        "rf.fit(X_train_count, y_train)\n",
        "y_pred = rf.predict(X_val_count)\n",
        "print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "# random forest with count vectors accuracy 0.7524084778420038\n",
        "\n",
        "#Below are 8 random statements. The first 4 depict happiness. The last 4 depict sadness\n",
        "\n",
        "tweets = pd.DataFrame(['I am very happy today! The atmosphere looks cheerful',\n",
        "'Things are looking great. It was such a good day',\n",
        "'Success is right around the corner. Lets celebrate this victory',\n",
        "'Everything is more beautiful when you experience them with a smile!',\n",
        "'Now this is my worst, okay? But I am gonna get better.',\n",
        "'I am tired, boss. Tired of being on the road, lonely as a sparrow in the rain. I am tired of all the pain I feel',\n",
        "'This is quite depressing. I am filled with sorrow',\n",
        "'His death broke my heart. It was a sad day'])\n",
        "\n",
        "# Doing some preprocessing on these tweets as done before\n",
        "tweets[0] = tweets[0].str.replace(r'[^\\w\\s]',' ',regex=True)\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "from textblob import Word\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "# Extracting Count Vectors feature from our tweets\n",
        "tweet_count = count_vect.transform(tweets[0])\n",
        "\n",
        "#Predicting the emotion of the tweet using our already trained linear SVM\n",
        "tweet_pred = lsvm.predict(tweet_count)\n",
        "print(tweet_pred)\n",
        "## result\n",
        "## [0 0 0 0 1 1 1 1]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}